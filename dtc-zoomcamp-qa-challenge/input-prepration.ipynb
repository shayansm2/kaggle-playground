{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98978ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe210a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_prepration import get_training_set_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13ecd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_training_set_data(seed=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293b3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, names='idx')\n",
    "df.drop(columns=['question_row_id', 'answer_row_id', 'question_id', 'answer_id'], inplace=True)\n",
    "# df['label'] = df['label'].astype(str)\n",
    "# df['idx'] = df['idx'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2544e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(df, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a63d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>148454</td>\n",
       "      <td>Why are you doing this for free? What motivate...</td>\n",
       "      <td>Ankush\\nAs I said, for me, it was Alexey's mac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>141290</td>\n",
       "      <td>Do you have separate materials or videos on se...</td>\n",
       "      <td>I do not have anything like that, but feel fre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>105325</td>\n",
       "      <td>How relevant is Terraform in a data engineerin...</td>\n",
       "      <td>Victoria\\nSQL is definitely super important. I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>135096</td>\n",
       "      <td>In the project, if we use Spark for transforma...</td>\n",
       "      <td>I wouldn't look at train at all, to be honest....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>26268</td>\n",
       "      <td>Linux Mint works way better than Ubuntu. It is...</td>\n",
       "      <td>Yeah. If you want to experiment with Linux, yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>34197</td>\n",
       "      <td>How does one move from a junior data engineer ...</td>\n",
       "      <td>Ankush\\nWhat we use is basically unit testing....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2527</th>\n",
       "      <td>114019</td>\n",
       "      <td>If Kubernetes can spin up containers when need...</td>\n",
       "      <td>Yes, we are.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2952</th>\n",
       "      <td>138977</td>\n",
       "      <td>If I have Kafka running on Virtual Machine 1 a...</td>\n",
       "      <td>Alexey\\nYes. That's the homework and the first...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>94638</td>\n",
       "      <td>Are data engineers expected to know Redshift, ...</td>\n",
       "      <td>Ankush\\nIf you're talking specifically about i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>115405</td>\n",
       "      <td>What is a landing zone?</td>\n",
       "      <td>It really depends on the cloud. I showed you h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3457 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                           question  \\\n",
       "1750  148454  Why are you doing this for free? What motivate...   \n",
       "2866  141290  Do you have separate materials or videos on se...   \n",
       "2036  105325  How relevant is Terraform in a data engineerin...   \n",
       "4268  135096  In the project, if we use Spark for transforma...   \n",
       "2532   26268  Linux Mint works way better than Ubuntu. It is...   \n",
       "...      ...                                                ...   \n",
       "989    34197  How does one move from a junior data engineer ...   \n",
       "2527  114019  If Kubernetes can spin up containers when need...   \n",
       "2952  138977  If I have Kafka running on Virtual Machine 1 a...   \n",
       "356    94638  Are data engineers expected to know Redshift, ...   \n",
       "2933  115405                            What is a landing zone?   \n",
       "\n",
       "                                                 answer  label  \n",
       "1750  Ankush\\nAs I said, for me, it was Alexey's mac...      1  \n",
       "2866  I do not have anything like that, but feel fre...      1  \n",
       "2036  Victoria\\nSQL is definitely super important. I...      1  \n",
       "4268  I wouldn't look at train at all, to be honest....      0  \n",
       "2532  Yeah. If you want to experiment with Linux, yo...      1  \n",
       "...                                                 ...    ...  \n",
       "989   Ankush\\nWhat we use is basically unit testing....      1  \n",
       "2527                                       Yes, we are.      0  \n",
       "2952  Alexey\\nYes. That's the homework and the first...      1  \n",
       "356   Ankush\\nIf you're talking specifically about i...      1  \n",
       "2933  It really depends on the cloud. I showed you h...      0  \n",
       "\n",
       "[3457 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea689574",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f11684b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7592, 102, 2178, 2742, 102, 0], [101, 7632, 102, 102, 0, 0, 0], [101, 7592, 7632, 102, 8909, 2243, 102]], 'token_type_ids': [[0, 0, 0, 1, 1, 1, 0], [0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1]], 'labels': [[101, 4931, 102, 0], [101, 2017, 102, 0], [101, 8909, 2243, 102]]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    ['hello','hi','hello hi'], \n",
    "    ['another example', '', 'IDK'], \n",
    "    ['hey', 'you', 'idk'], \n",
    "    padding = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51853a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(\n",
    "    list(train['question']),\n",
    "    list(train['answer']),\n",
    "    padding = True, truncation=True    \n",
    ")\n",
    "\n",
    "validation_tokenized = tokenizer(\n",
    "    list(validation['question']),\n",
    "    list(validation['answer']),\n",
    "    padding = True, truncation=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e25d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "032561b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\") # ddecrease learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aab9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_tokenized, train['label']))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_tokenized, validation['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20158141",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = np.array(train_tokenized[\"input_ids\"])\n",
    "train_labels = tf.constant(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3108b67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  2/109 [..............................] - ETA: 1:33:36 - loss: 2.3863"
     ]
    }
   ],
   "source": [
    "# model.fit(train_dataset, epochs=10, validation_data=validation_dataset)\n",
    "# labels = tf.convert_to_tensor(train['label'])\n",
    "# model.fit(**train_tokenized, labels=labels)\n",
    "model.fit(train_encodings, train_labels, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99a38b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 6s 6s/step - loss: 4.4525 - accuracy: 0.5000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 4.5617 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2e15c2980>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Prepare the data\n",
    "train_encodings = tokenizer([\"Hello, world!\", \"How are you?\"], truncation=True, padding=True)\n",
    "train_labels = tf.constant([1, 0])\n",
    "train_encodings = np.array(train_encodings[\"input_ids\"])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_encodings, train_labels, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50b1b98d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(train_tokenized)\n\u001b[1;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/keras/src/engine/training.py:2783\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   2782\u001b[0m ):\n\u001b[0;32m-> 2783\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_batch_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2784\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\n\u001b[1;32m   2785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2786\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m   2787\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1940\u001b[0m, in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1938\u001b[0m     data \u001b[38;5;241m=\u001b[39m (x, y, sample_weight)\n\u001b[0;32m-> 1940\u001b[0m \u001b[43m_check_data_cardinality\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1941\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensors(data)\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_weight:\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1949\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_data_cardinality\u001b[39m(data):\n\u001b[0;32m-> 1949\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(num_samples) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1951\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData cardinality is ambiguous:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py10/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1949\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_data_cardinality\u001b[39m(data):\n\u001b[0;32m-> 1949\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(data))\n\u001b[1;32m   1950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(num_samples) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1951\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData cardinality is ambiguous:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "batch = dict(train_tokenized)\n",
    "labels = tf.convert_to_tensor(train['label'])\n",
    "model.train_on_batch(batch, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c60f601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
