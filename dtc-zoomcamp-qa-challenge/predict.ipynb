{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = 'For the project, how long will be the timeframe to complete it?'\n",
    "ans1 = \"Alexey\\nYou can just take a look at the thread with datasets that we had in Slack and I’m pretty sure you will find something interesting there. Then just go from there. I would start from a dataset. First, I would find the dataset and then try to think what exactly I can do with this dataset.\\nAnkush\\nI can say that if you finish one project where you are basically using more batch processing, then maybe the next project can be more real-time processing. Similarly if you're doing less Airflow, then do more Airflow. Something like that. Try to explore different areas to broaden your knowledge.\\nSejal\\nNothing else to add from my end, really. Basically just what you guys said. I think it really depends on the needs are. The situation is based on what you feel like learning, what you think is important for you to learn in order to implement, with respect to what you're currently working on. I think you'll figure it out. In addition to what you're learning in this course, maybe you could also try learning some standard software engineering skills on your own, which can be applied in data engineering as well, such as orchestrating CI/CD pipelines, workflows, GitOps, automated test cases, especially unit tests, maybe with pytest. You can use pytest in combination with Airflow. Make your code more production-friendly, basically. We haven't really covered that. I'm not sure if we will be covering these modules in the next iterations. If we find the time, then definitely we will add it as bonus lectures. This is also something you can try out.\"\n",
    "label1 = 0\n",
    "\n",
    "q2 = 'For hyperparameter tuning, should I be tuning using the full train and test dataset?'\n",
    "ans2 = \"Yes. If you're running on Windows, what you can do is first install WSL (Windows Subsystem for Linux) and then use it. The second thing you can do is just altboot and install Ubuntu on your computer. This way, you can keep Windows and then for, let's say, development work, you just go and use Linux. Then every time you don't know how to do something, you Google it. At least this is how I learned Linux. I think you will be able to learn it this way as well.\\xa0\\nWhat you can do is – you have a command line, so just try to do as much as possible without leaving the command line. For example, say In this is Ls. If you want to look inside some file, you use “less”. Get familiar with tools like these simple Unix tools, like ls, less, CD, vd and so on – things like this. I think that should be sufficient for most of your work.\"\n",
    "label2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1 = tokenizer(q1, ans1, padding=True, truncation=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.07220709, 0.02704169]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(**tokens1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-1.4144058,  1.383347 ]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2 = tokenizer(q2, ans2, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "res = model(**tokens2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = 'Is time not a categorical variable because it has only 11 values?'\n",
    "ans3 = \"Dmitry\\nUsually, if you're working with linear models, for sure, you need to have the same scale. It’s a good question, but it wasn't covered in this homework due to us trying to make it a bit simpler at the initial stages. But for sure, this is very important to know if you're working with the linear model, you need to have the same scale.\\xa0\\nAlexey\\nHere, we use normal equation, and for normal equation, maybe it matters less. Of course, if your features vary significantly, if in one case you have features that are in billions and another is between zero and one, then you probably need to do some normalization. But I think in cases like mileage and engine horsepower – yes, they are different, but for normal equation that we used here, it's not as important as for other types of finding solutions. We didn't talk about stochastic gradient descent or gradient descent in this course (in general). We will talk a bit about this in neural networks. But for this type of finding solution, there you need to scale features. Not for normal equation, though. Normal equation, you will be fine most of the time, I think, except if there is a huge discrepancy.\\xa0\\nDmitry\\nYeah, for sure it will affect it. But, as you said, if there is a huge difference, then definitely.\\xa0\\nAlexey\\nYeah, I saw this question multiple times already, by the way, about stochastic gradient descent. This course from Andrew Ng – I don't remember whether he starts with normal equation or with gradient descent or the other way around. But today, Dmitry shared a good article with me. Maybe you can share it in the channels? Well, I think it's a nice one. If you're interested in other ways of finding solutions to linear regression, that's a good read. For that (for stochastic gradient descent) you need to normalize features.\"\n",
    "label3 = 0\n",
    "\n",
    "q4 = 'There are many ways to write codes. Are short codes better in actual jobs as a machine learning beginner? Should I focus on writing concise codes? What is production code?'\n",
    "ans4 = 'Dmitry\\nUsually I use the NumPy function.\\xa0\\nAlexey\\nI think also the question is, “Do we need to transform the y variable in any way apart from this logarithmic transformation? Are there other transformations that we need to know?”\\nDmitry\\nI can think of the Box Cox transformation. There are certain types of transformations of the target variable, especially in the regression tasks. You have to make it more randomly distributed, for example. For sure, you can leave it like that, but the results, especially in a linear model because linear models really depend on the linearity, or the normal distribution of the target variable, so the results can be not very good.\\xa0\\nFor sure, throughout the course of the Zoomcamp, we will talk about the nonlinear methods such as end symbols, for example, end there. You can see that, for example, you can use it without transformation – using the trees.\\nAlexey\\nYeah, thanks. “Cox Box transformation” you said? [Dmitry agrees] I think this one turns any variable into a normal, right? [Dmitry agrees]'\n",
    "label4 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.05744573, 0.9425543 ]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(res.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at model.h5 were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py_scripts.model_interface import predict, predict_probabilities\n",
    "\n",
    "predict(q3, ans3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.03661787137389183, 0.9633820652961731]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_probabilities(q4, ans4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
