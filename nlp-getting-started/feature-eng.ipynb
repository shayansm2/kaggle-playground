{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "192893fbc2f5da6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## import libs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91af6110e06160f0"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:13:48.297227Z",
     "start_time": "2023-11-03T10:13:48.076524Z"
    }
   },
   "id": "aa15176bc7f07220"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## prepare data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac27448df313ab6b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', index_col='id')\n",
    "df['words_count'] = df.text.apply(len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T08:57:06.038278Z",
     "start_time": "2023-11-03T08:57:06.023006Z"
    }
   },
   "id": "118192591947d046"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def text_process(mess: str):\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    nopunc = nopunc.lower().strip()\n",
    "\n",
    "    # Now just remove any stopwords\n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])\n",
    "\n",
    "\n",
    "df['clean_text'] = df['text'].apply(text_process)\n",
    "df['clean_words_count'] = df['clean_text'].apply(len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T13:24:28.568975Z",
     "start_time": "2023-11-02T13:24:24.138681Z"
    }
   },
   "id": "2eb3e3555ed33eab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## add some features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1051aaabecaa7af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "because location column has a lot of missing values (49%) we will use the has_location instead. after that we will delete this column "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1c1cb4b9cf6b93"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df['has_location'] = df['location'].notnull()\n",
    "del df['location']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T13:24:28.589168Z",
     "start_time": "2023-11-02T13:24:28.574452Z"
    }
   },
   "id": "8556a185a4c491d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "additionally these are other symbols, which are in the text and can be used as a mean to predict whether the tweet is disaster or not."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eb037e54bed2d45"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df['has_question_mark'] = df['text'].str.contains('\\?').astype(int)\n",
    "df['has_exclamation_mark'] = df['text'].str.contains('\\!').astype(int)\n",
    "df['has_hashtag'] = df['text'].str.contains('\\#').astype(int)\n",
    "df['has_capital_words'] = df['text'].apply(lambda x: str(x).isupper()).astype(int)\n",
    "df['has_link'] = df['text'].str.contains(\n",
    "    'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+').astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T13:24:28.594936Z",
     "start_time": "2023-11-02T13:24:28.578662Z"
    }
   },
   "id": "ebf1211c507d6286"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   keyword                                               text  target  \\\nid                                                                      \n1      NaN  Our Deeds are the Reason of this #earthquake M...       1   \n4      NaN             Forest fire near La Ronge Sask. Canada       1   \n5      NaN  All residents asked to 'shelter in place' are ...       1   \n6      NaN  13,000 people receive #wildfires evacuation or...       1   \n7      NaN  Just got sent this photo from Ruby #Alaska as ...       1   \n\n    words_count                                         clean_text  \\\nid                                                                   \n1            69       deeds reason earthquake may allah forgive us   \n4            38              forest fire near la ronge sask canada   \n5           133  residents asked shelter place notified officer...   \n6            65  13000 people receive wildfires evacuation orde...   \n7            88  got sent photo ruby alaska smoke wildfires pou...   \n\n    clean_words_count  has_location  has_question_mark  has_exclamation_mark  \\\nid                                                                             \n1                  44         False                  0                     0   \n4                  37         False                  0                     0   \n5                  88         False                  0                     0   \n6                  59         False                  0                     0   \n7                  55         False                  0                     0   \n\n    has_hashtag  has_capital_words  has_link  \nid                                            \n1             1                  0         0  \n4             0                  0         0  \n5             0                  0         0  \n6             1                  0         0  \n7             1                  0         0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>text</th>\n      <th>target</th>\n      <th>words_count</th>\n      <th>clean_text</th>\n      <th>clean_words_count</th>\n      <th>has_location</th>\n      <th>has_question_mark</th>\n      <th>has_exclamation_mark</th>\n      <th>has_hashtag</th>\n      <th>has_capital_words</th>\n      <th>has_link</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>69</td>\n      <td>deeds reason earthquake may allah forgive us</td>\n      <td>44</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>38</td>\n      <td>forest fire near la ronge sask canada</td>\n      <td>37</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>133</td>\n      <td>residents asked shelter place notified officer...</td>\n      <td>88</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>65</td>\n      <td>13000 people receive wildfires evacuation orde...</td>\n      <td>59</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>88</td>\n      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n      <td>55</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T13:24:28.626298Z",
     "start_time": "2023-11-02T13:24:28.594157Z"
    }
   },
   "id": "d501b39ece70cb6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "in order to work with text data, we should convert them into numerical features so that they can be understood by the machine learning models. Like `DictVectorizer` from `sklearn` package which converts enum columns into numerical features, `CountVectorizer` can be used inorder to convert text data into numerical features. Each words have its own column/feature and if that word exists in a row, the value will be 1, otherwise 0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ede783a283bae858"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(df['clean_text'])\n",
    "representation = vect.transform(df['clean_text']).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T13:27:14.178183Z",
     "start_time": "2023-11-02T13:27:14.048629Z"
    }
   },
   "id": "16f56929e732e656"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n"
     ]
    },
    {
     "data": {
      "text/plain": "array(['allah', 'deeds', 'earthquake', 'forgive', 'may', 'reason', 'us'],\n      dtype=object)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.iloc[0]['text'])\n",
    "firstRowText = representation[0]\n",
    "tokenIndices = [i for i, x in enumerate(firstRowText) if x == 1]\n",
    "vect.get_feature_names_out()[tokenIndices]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T13:33:51.656079Z",
     "start_time": "2023-11-02T13:33:51.645115Z"
    }
   },
   "id": "314eae895e27ef31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "as we see the first row text was \"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
    "after removing the stop words, the tokens are \"allah\", \"deeds\", \"earthquake\", \"forgive\", \"may\", \"reason\" and \"us\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b65dcc43507c442c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train, validation and test data sets split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d78069ee5900c91"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:13:51.296788Z",
     "start_time": "2023-11-03T10:13:51.256232Z"
    }
   },
   "id": "b4e1b1770f36275f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state_seed = 22\n",
    "df_train_validation, df_test = train_test_split(df, test_size=0.2, random_state=random_state_seed)\n",
    "df_train, df_validation = train_test_split(df_train_validation, test_size=0.25, random_state=random_state_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:13:51.667712Z",
     "start_time": "2023-11-03T10:13:51.662561Z"
    }
   },
   "id": "9ffa425cc5df5742"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### converting data frame to desired input of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3193f110804a11"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class InputProvider(object):\n",
    "    def get_train_inputs(self, df: pd.DataFrame) -> tuple:\n",
    "        pass\n",
    "\n",
    "    def get_test_inputs(self, df: pd.DataFrame) -> tuple:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:13:52.863759Z",
     "start_time": "2023-11-03T10:13:52.818361Z"
    }
   },
   "id": "f3890c2350c3201e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_clean_text(mess: str):\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    nopunc = nopunc.lower().strip()\n",
    "\n",
    "    # Now just remove any stopwords\n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:13:53.426824Z",
     "start_time": "2023-11-03T10:13:53.394191Z"
    }
   },
   "id": "5a27cfc215032b35"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def add_new_features_from_text(df_original: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_original.copy()\n",
    "    df['words_count'] = df.text.apply(len)\n",
    "\n",
    "    df['has_location'] = df['location'].notnull().astype(int)\n",
    "    del df['location']\n",
    "    df['has_question_mark'] = df['text'].str.contains('\\?').astype(int)\n",
    "    df['has_exclamation_mark'] = df['text'].str.contains('\\!').astype(int)\n",
    "    df['has_hashtag'] = df['text'].str.contains('\\#').astype(int)\n",
    "    df['has_capital_words'] = df['text'].apply(lambda x: str(x).isupper()).astype(int)\n",
    "    df['has_link'] = df['text'].str.contains(\n",
    "        'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+').astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_clean_text_features(df_original: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_original.copy()\n",
    "    df['clean_text'] = df['text'].apply(get_clean_text)\n",
    "    df['clean_words_count'] = df['clean_text'].apply(len)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:13:54.114712Z",
     "start_time": "2023-11-03T10:13:54.110308Z"
    }
   },
   "id": "13d0d11f86888d0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "InputProvider1 feature list:\n",
    "- words_count\n",
    "- has_location\n",
    "- has_question_mark\n",
    "- has_exclamation_mark\n",
    "- has_hashtag\n",
    "- has_capital_words\n",
    "- has_link\n",
    "\n",
    "dimension input matrix: n_rows * 7"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bae4f01e3a7488cb"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class InputProvider1(InputProvider):\n",
    "    @staticmethod\n",
    "    def _get_input_base(df: pd.DataFrame) -> tuple:\n",
    "        df = add_new_features_from_text(df)\n",
    "        y = df.target\n",
    "        df.drop(columns=['id', 'text', 'keyword', 'target'], inplace=True)\n",
    "        x = df.values\n",
    "        return x, y\n",
    "\n",
    "    def get_train_inputs(self, df: pd.DataFrame) -> tuple:\n",
    "        return self._get_input_base(df)\n",
    "\n",
    "    def get_test_inputs(self, df: pd.DataFrame) -> tuple:\n",
    "        return self._get_input_base(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:13:55.380102Z",
     "start_time": "2023-11-03T10:13:55.376639Z"
    }
   },
   "id": "52039e26764cf52d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(4567, 7)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(InputProvider1().get_train_inputs(df_train)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:13:56.085068Z",
     "start_time": "2023-11-03T10:13:56.041693Z"
    }
   },
   "id": "5544070dcc059290"
  },
  {
   "cell_type": "markdown",
   "source": [
    "InputProvider2 feature list:\n",
    "- words_count\n",
    "- has_location\n",
    "- has_question_mark\n",
    "- has_exclamation_mark\n",
    "- has_hashtag\n",
    "- has_capital_words\n",
    "- has_link\n",
    "- keywords **(one hot encoding)** \n",
    "\n",
    "dimension input matrix: n_rows * 229"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15cfc97a7cbcc4cf"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "class InputProvider2(InputProvider):\n",
    "    def __init__(self):\n",
    "        self.vect = DictVectorizer()\n",
    "\n",
    "    def get_train_inputs(self, df: pd.DataFrame) -> tuple:\n",
    "        df = add_new_features_from_text(df)\n",
    "        y = df.target\n",
    "        df.drop(columns=['id', 'text', 'target'], inplace=True)\n",
    "        self.vect.fit(df.to_dict(orient='records'))\n",
    "        x = self.vect.transform(df.to_dict(orient='records'))\n",
    "        return x, y\n",
    "\n",
    "    def get_test_inputs(self, df: pd.DataFrame) -> tuple:\n",
    "        df = add_new_features_from_text(df)\n",
    "        y = df.target\n",
    "        df.drop(columns=['id', 'text', 'target'], inplace=True)\n",
    "        x = self.vect.transform(df.to_dict(orient='records'))\n",
    "        return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:13:58.187723Z",
     "start_time": "2023-11-03T10:13:58.160050Z"
    }
   },
   "id": "41529f5d085aa4a7"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(4567, 229)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(InputProvider2().get_train_inputs(df_train)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:13:58.772259Z",
     "start_time": "2023-11-03T10:13:58.736710Z"
    }
   },
   "id": "2d6eed02495ba33a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "InputProvider3 feature list:\n",
    "- clean text tokens\n",
    "\n",
    "dimension input matrix: n_rows * 15699"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c25a1d36f4ab0fb"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "class InputProvider3(InputProvider):\n",
    "    def __init__(self):\n",
    "        self.vect = CountVectorizer()\n",
    "\n",
    "    def get_train_inputs(self, df: pd.DataFrame) -> tuple:\n",
    "        df = add_clean_text_features(df)\n",
    "        y = df.target\n",
    "        self.vect.fit(df['clean_text'])\n",
    "        x = self.vect.transform(df['clean_text'])\n",
    "        return x, y\n",
    "\n",
    "    def get_test_inputs(self, df: pd.DataFrame) -> tuple:\n",
    "        df = add_clean_text_features(df)\n",
    "        y = df.target\n",
    "        x = self.vect.transform(df['clean_text'])\n",
    "        return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:14:00.199613Z",
     "start_time": "2023-11-03T10:14:00.181779Z"
    }
   },
   "id": "5eb01f00bfce94fa"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(4567, 15699)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(InputProvider3().get_train_inputs(df_train)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:14:03.668530Z",
     "start_time": "2023-11-03T10:14:00.980237Z"
    }
   },
   "id": "c65ee2678744b11b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "InputProvider4 feature list:\n",
    "- words_count\n",
    "- has_location\n",
    "- has_question_mark\n",
    "- has_exclamation_mark\n",
    "- has_hashtag\n",
    "- has_capital_words\n",
    "- has_link\n",
    "- clean text tokens \n",
    "\n",
    "dimension input matrix: n_rows * 15707"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db6b3b0a79263ca0"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "\n",
    "class InputProvider4(InputProvider):\n",
    "    def __init__(self):\n",
    "        self.vect = CountVectorizer()\n",
    "\n",
    "    def get_train_inputs(self, df: pd.DataFrame) -> tuple:\n",
    "        df = add_new_features_from_text(add_clean_text_features(df))\n",
    "        y = df.target\n",
    "        self.vect.fit(df['clean_text'])\n",
    "        tokens = self.vect.transform(df['clean_text'])\n",
    "        sparce_features = csr_matrix(df.drop(columns=['text', 'clean_text', 'keyword', 'id', 'target']).values)\n",
    "        x = hstack([tokens, sparce_features])\n",
    "        return x, y\n",
    "\n",
    "    def get_test_inputs(self, df: pd.DataFrame) -> tuple:\n",
    "        df = add_new_features_from_text(add_clean_text_features(df))\n",
    "        y = df.target\n",
    "        tokens = self.vect.transform(df['clean_text'])\n",
    "        sparce_features = csr_matrix(df.drop(columns=['text', 'clean_text', 'keyword', 'id', 'target']).values)\n",
    "        x = hstack([tokens, sparce_features])\n",
    "        return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:21:05.870340Z",
     "start_time": "2023-11-03T10:21:05.852791Z"
    }
   },
   "id": "8448a6e2eff5401d"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(4567, 15707)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(InputProvider4().get_train_inputs(df_train)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:21:09.654853Z",
     "start_time": "2023-11-03T10:21:06.946232Z"
    }
   },
   "id": "eb1d9abf0dd40081"
  },
  {
   "cell_type": "markdown",
   "source": [
    "InputProvider5 feature list:\n",
    "- words_count\n",
    "- has_location\n",
    "- has_question_mark\n",
    "- has_exclamation_mark\n",
    "- has_hashtag\n",
    "- has_capital_words\n",
    "- has_link\n",
    "- clean text tokens \n",
    "- keywords\n",
    "\n",
    "dimension input matrix: n_rows *"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26fca7c6479ad395"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class InputProvider5(InputProvider):\n",
    "    def __init__(self):\n",
    "        self.count_vect = CountVectorizer()\n",
    "        self.dict_vect = DictVectorizer()\n",
    "\n",
    "    def get_train_inputs(self, df: pd.DataFrame) -> tuple:\n",
    "        df = add_new_features_from_text(add_clean_text_features(df))\n",
    "        y = df.target\n",
    "        self.count_vect.fit(df['clean_text'])\n",
    "        tokens = self.count_vect.transform(df['clean_text'])\n",
    "        sparce_features = csr_matrix(df.drop(columns=['text', 'clean_text', 'keyword', 'id', 'target']).values)\n",
    "        x = hstack([tokens, sparce_features])\n",
    "        return x, y\n",
    "\n",
    "    def get_test_inputs(self, df: pd.DataFrame) -> tuple:\n",
    "        df = add_new_features_from_text(add_clean_text_features(df))\n",
    "        y = df.target\n",
    "        tokens = self.count_vect.transform(df['clean_text'])\n",
    "        sparce_features = csr_matrix(df.drop(columns=['text', 'clean_text', 'keyword', 'id', 'target']).values)\n",
    "        x = hstack([tokens, sparce_features])\n",
    "        return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T10:22:16.722583Z",
     "start_time": "2023-11-03T10:22:16.672928Z"
    }
   },
   "id": "a6cefa51af98316"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d19ec3eb5af20878"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
