{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "192893fbc2f5da6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## import libs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91af6110e06160f0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T12:39:11.908601Z",
     "start_time": "2023-10-30T12:39:11.898772Z"
    }
   },
   "id": "aa15176bc7f07220"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## prepare data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac27448df313ab6b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', index_col='id')\n",
    "df['words_count'] = df.text.apply(len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T12:39:14.396895Z",
     "start_time": "2023-10-30T12:39:14.370134Z"
    }
   },
   "id": "118192591947d046"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def text_process(mess: str):\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    nopunc = nopunc.lower().strip()\n",
    "\n",
    "    # Now just remove any stopwords\n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])\n",
    "\n",
    "\n",
    "df['clean_text'] = df['text'].apply(text_process)\n",
    "df['clean_words_count'] = df['clean_text'].apply(len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T12:39:19.565661Z",
     "start_time": "2023-10-30T12:39:15.118630Z"
    }
   },
   "id": "2eb3e3555ed33eab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## add some features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1051aaabecaa7af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "because location column has a lot of missing values (49%) we will use the has_location instead. after that we will delete this column "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1c1cb4b9cf6b93"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df['has_location'] = df['location'].notnull()\n",
    "del df['location']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T12:39:19.578240Z",
     "start_time": "2023-10-30T12:39:19.567010Z"
    }
   },
   "id": "8556a185a4c491d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "additionally these are other symbols, which are in the text and can be used as a mean to predict whether the tweet is disaster or not."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9eb037e54bed2d45"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df['has_question_mark'] = df['text'].str.contains('\\?').astype(int)\n",
    "df['has_exclamation_mark'] = df['text'].str.contains('\\!').astype(int)\n",
    "df['has_hashtag'] = df['text'].str.contains('\\#').astype(int)\n",
    "df['has_capital_words'] = df['text'].apply(lambda x: str(x).isupper()).astype(int)\n",
    "df['has_link'] = df['text'].str.contains(\n",
    "    'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+').astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T12:39:19.590637Z",
     "start_time": "2023-10-30T12:39:19.582547Z"
    }
   },
   "id": "ebf1211c507d6286"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "   keyword                                               text  target  \\\nid                                                                      \n1      NaN  Our Deeds are the Reason of this #earthquake M...       1   \n4      NaN             Forest fire near La Ronge Sask. Canada       1   \n5      NaN  All residents asked to 'shelter in place' are ...       1   \n6      NaN  13,000 people receive #wildfires evacuation or...       1   \n7      NaN  Just got sent this photo from Ruby #Alaska as ...       1   \n\n    words_count                                         clean_text  \\\nid                                                                   \n1            69       deeds reason earthquake may allah forgive us   \n4            38              forest fire near la ronge sask canada   \n5           133  residents asked shelter place notified officer...   \n6            65  13000 people receive wildfires evacuation orde...   \n7            88  got sent photo ruby alaska smoke wildfires pou...   \n\n    clean_words_count  has_location  has_question_mark  has_exclamation_mark  \\\nid                                                                             \n1                  44         False                  0                     0   \n4                  37         False                  0                     0   \n5                  88         False                  0                     0   \n6                  59         False                  0                     0   \n7                  55         False                  0                     0   \n\n    has_hashtag  has_capital_words  has_link  \nid                                            \n1             1                  0         0  \n4             0                  0         0  \n5             0                  0         0  \n6             1                  0         0  \n7             1                  0         0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keyword</th>\n      <th>text</th>\n      <th>target</th>\n      <th>words_count</th>\n      <th>clean_text</th>\n      <th>clean_words_count</th>\n      <th>has_location</th>\n      <th>has_question_mark</th>\n      <th>has_exclamation_mark</th>\n      <th>has_hashtag</th>\n      <th>has_capital_words</th>\n      <th>has_link</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>69</td>\n      <td>deeds reason earthquake may allah forgive us</td>\n      <td>44</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>38</td>\n      <td>forest fire near la ronge sask canada</td>\n      <td>37</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>133</td>\n      <td>residents asked shelter place notified officer...</td>\n      <td>88</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>65</td>\n      <td>13000 people receive wildfires evacuation orde...</td>\n      <td>59</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>88</td>\n      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n      <td>55</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T12:39:19.594363Z",
     "start_time": "2023-10-30T12:39:19.588258Z"
    }
   },
   "id": "d501b39ece70cb6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "in order to work with text data, we should convert them into numerical features so that they can be understood by the machine learning models. Like `DictVectorizer` from `sklearn` package which converts enum columns into numerical features, `CountVectorizer` can be used inorder to convert text data into numerical features. Each words have its own column/feature and if that word exists in a row, the value will be 1, otherwise 0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ede783a283bae858"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<7613x22310 sparse matrix of type '<class 'numpy.int64'>'\n\twith 73854 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(df['clean_text'])\n",
    "vect.transform(df['clean_text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T12:40:19.841859Z",
     "start_time": "2023-10-30T12:40:19.757393Z"
    }
   },
   "id": "16f56929e732e656"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "314eae895e27ef31"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
